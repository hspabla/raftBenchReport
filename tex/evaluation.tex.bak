\section{Evaluation}

\subsection{Experiment Setup}

In our experiments, we ran three regions with three servers in each region. Each region is running its own Raft consensus group. 

To test without observers, we pass a runtime flag to our program that disables the creation of the $m-1$ observer \texttt{appendEntries} threads. As a result, the notion of observer is virtually non-existant. We then have our analytics client fetch logs from each region's leader and call \texttt{resolveLogs}.

To test with observers, we only need our analytics client to fetch the latest snapshot from the local leader (serving as local observer).

Since our primary goal is to improve the performance of generating snapshots with observers, we propose two metrics: throughput and staleness (see Table~\ref{tab:benchmark}). Throughput will be measured in the average number of snapshots that we can generate per minute. Staleness will be measured the average age of the snapshot contents relative to the actual content on the remote servers.

To calculate the age of the snapshot, we assume that the clocks on all of our machines will not drift apart too much during our experiment. Each leader will periodically generate a new key-value entry in its log with its region id as key and its machine's UNIX time as value. Once we resolve the vector timestamps in all the logs, we look for the largest difference between each key-value entry and the local region's key-value entry (since the local region is the most up-to-date and should have the largest UNIX time value).

\subsection{Results}

\begin{table}[ht]
\centering
\caption{Benchmark Comparison}
\label{tab:benchmark}
\begin{tabular}{|l|c|c|} \hline
           & No Observers & With Observers \\ \hline
Throughput &              &                \\ \hline
Staleness  &              &                \\ \hline
\end{tabular}
\end{table}

